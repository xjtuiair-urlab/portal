---
import SidebarLayout from '../layouts/SidebarLayout.astro';

const rawBase = import.meta.env.BASE_URL;
const baseUrl = rawBase.endsWith('/') ? rawBase.slice(0, -1) : rawBase;
const getLink = (path: string) => {
    const cleanPath = path.startsWith('/') ? path.slice(1) : path;
    if (cleanPath === '') return `${baseUrl}/`;
    return `${baseUrl}/${cleanPath}`;
}
---

<SidebarLayout title="Overview of Our Research" bannerTitle="Research" category="research">
    
    <div class="research-list">
        <!-- Embodied AI Card -->
        <div class="research-card" id="embodied">
            <div class="card-main">
                <div class="card-image">
                    <img src={getLink("assets/background/embodiedai.png")} alt="Embodied AI" class="placeholder-img" />
                </div>
                <div class="card-info">
                    <h2 class="card-title">Embodied AI</h2>
                    <p class="summary">
                        As artificial intelligence systems increasingly interact with the physical world, 
                        the field of Embodied AI focuses on developing agents that can perceive, reason, 
                        and act within dynamic environments.
                    </p>
                    <a href="#embodied" class="read-more">Read More ➝</a>
                </div>
            </div>
            <div class="card-details">
                <p>
                    Moving beyond static datasets, it tackles the 
                    core challenge of integrating sensory understanding with physical action to achieve 
                    complex tasks. Covering areas from robotic navigation to interactive task completion, 
                    our core exploration centers on building intelligent systems that learn from and adapt 
                    to the real world through embodied experiences.
                </p>
                <h3 class="content-subtitle">Vision Language Action Models</h3>
                <p>
                    The integration of vision, language, and action represents a frontier in creating generalist 
                    robotic agents. Vision-Language-Action (VLA) models are challenged by the need to ground 
                    linguistic instructions and visual perception into precise, sequential physical actions. 
                    Spanning the domains of multimodal learning, robot control, and real-world deployment.
                </p>
                <div class="media-grid">
                    <video controls><source src={getLink("assets/research/embodied1.mp4")} type="video/mp4"></video>
                    <video controls><source src={getLink("assets/research/embodied2.mp4")} type="video/mp4"></video>
                    <video controls><source src={getLink("assets/research/embodied3.mp4")} type="video/mp4"></video>
                </div>
                <h3 class="content-subtitle">Vision Language Models</h3>
                <p>
                    Vision-Language Models (VLMs) aim to bridge visual perception and natural language understanding, 
                    enabling machines to interpret, reason about, and communicate complex visual information. 
                    By jointly modeling images and text, VLMs move beyond isolated visual recognition or 
                    language processing, addressing the fundamental challenge of multimodal grounding and alignment. 
                    This paradigm supports a wide range of capabilities, from visual question answering and image 
                    captioning to multimodal reasoning and instruction following. Our research 
                    advances VLMs through scalable pretraining, robust cross-modal grounding, and efficient adaptation 
                    to downstream embodied applications, bridging the gap between passive understanding and active engagement 
                    with the world.
                </p>
            </div>
        </div>

        <!-- Emerging Computing Architecture Card -->
        <div class="research-card" id="newarch">
            <div class="card-main">
                <div class="card-image">
                    <img src={getLink("assets/research/cim.png")} alt="Architecture" />
                </div>
                <div class="card-info">
                    <h2 class="card-title">Emerging Computing Architectures</h2>
                    <p class="summary">
                        As modern artificial intelligence systems continue to scale, traditional computing 
                        architectures are increasingly constrained by efficiency, latency, and energy consumption.
                    </p>
                    <a href="#newarch" class="read-more">Read More ➝</a>
                </div>
            </div>
            <div class="card-details">
                <p>
                    Emerging computing architectures aim to fundamentally balance the interaction between 
                    algorithms, hardware, and systems, enabling 
                    efficient and scalable execution of deep learning and large language models across cloud, 
                    edge, and resource-constrained platforms. Our research explores architecture–algorithm 
                    co-design paradigms that bridge theoretical model advances with practical hardware realizations.
                </p>
                <h3 class="content-subtitle">Deep Learning and LLM Accelerators</h3>
                <p>
                    While deep learning and large language models have demonstrated remarkable success across 
                    a wide range of real-world applications, their rapidly growing computational and memory 
                    demands pose significant challenges to conventional hardware platforms. Most existing 
                    models are designed with limited awareness of hardware constraints, leading to inefficiencies 
                    in performance, energy consumption, and scalability. To address these challenges, we focus on 
                    the co-design of hardware-friendly model architectures and specialized accelerators, alongside 
                    system-level optimization techniques for efficient deployment. Our research spans accelerator 
                    microarchitecture, dataflow optimization, and software–hardware interfaces, aiming to enable 
                    high-performance and energy-efficient execution of deep learning and LLM workloads, 
                    particularly for edge and embedded environments.
                </p>
                <h3 class="content-subtitle">In-Memory and Near-Memory Computing</h3>
                <p>
                    The increasing dominance of data movement costs has exposed fundamental limitations of the 
                    traditional von Neumann architecture, especially for data-intensive AI workloads. In-memory 
                    and near-memory computing paradigms seek to alleviate this bottleneck by tightly integrating 
                    computation with memory, reducing data transfer overhead and improving energy efficiency. 
                    Our research investigates compute-in-memory architectures, mixed-signal and digital designs, 
                    and algorithm–hardware co-optimization strategies, with a particular emphasis on neural 
                    network inference and learning. By exploring novel memory devices, architectural abstractions, 
                    and system-level integration, we aim to unlock new pathways toward scalable and efficient 
                    AI computing beyond conventional architectures.
                </p>
                <div class="research-images">
                    <img src={getLink("assets/research/cim.png")} alt="CIM">
                    <img src={getLink("assets/research/Layer.png")} alt="Layer">
                    <img src={getLink("assets/background/cim.png")} alt="CIM Device">
                </div>
            </div>
        </div>

        <!-- Efficient AI Card -->
        <div class="research-card" id="efficientai">
            <div class="card-main">
                <div class="card-image">
                    <img src={getLink("assets/background/mllm.png")} alt="Architecture" />
                </div>
                <div class="card-info">
                    <h2 class="card-title">Efficient AI</h2>
                    <p class="summary">
                        Efficient AI addresses the critical challenge of maximizing model performance while 
                        minimizing computational cost, energy consumption, and memory footprint.
                    </p>
                    <a href="#efficientai" class="read-more">Read More ➝</a>
                </div>
            </div>
            <div class="card-details">
                <p>
                    As state-of-the-art AI models grow exponentially in size and resource requirements, 
                    their deployment in real-world, resource-constrained settings becomes profoundly difficult. 
                    Efficient AI addresses the critical 
                    challenge of maximizing model performance while minimizing computational cost, energy consumption, and memory 
                    footprint. Spanning techniques from algorithm design to hardware-aware optimization, our central mission is 
                    to develop methodologies that make advanced AI capabilities accessible, sustainable, and scalable.
                </p>
            </div>
        </div>
    </div>

</SidebarLayout>

<style>
    .research-list { display: flex; flex-direction: column; gap: 50px; } /* 增加卡片之间的间距从 30px 到 50px */
    
    .research-card {
        background: #fff;
        border-radius: 12px;
        overflow: hidden;
        box-shadow: 0 4px 15px rgba(0,0,0,0.05);
        border: 1px solid #eee;
        transition: 0.3s;
        scroll-margin-top: 120px;
    }

    .card-main { display: flex; gap: 30px; } /* 增加图片和文字之间的间距 */
    
    .card-image { width: 300px; height: 200px; flex-shrink: 0; background: #f5f5f5; }
    .card-image img { width: 100%; height: 100%; object-fit: cover; }
    .image-placeholder { display: flex; align-items: center; justify-content: center; height: 100%; color: #ccc; font-weight: bold; }

    .card-info { padding: 30px; display: flex; flex-direction: column; justify-content: center; } /* 增加内边距 */
    .card-title { font-size: 1.8rem; color: var(--primary-blue); margin-bottom: 15px; border: none; padding: 0; }
    .summary { font-size: 1rem; color: #666; margin-bottom: 20px; line-height: 1.6; }
    
    .read-more { color: var(--accent-orange); font-weight: 600; text-decoration: none; }

    .card-details { 
        display: none; 
        padding: 40px 30px 30px; /* 增加展开后的顶部间距，从 0 增加到 40px */
        border-top: 1px solid #eee;
        margin-top: 10px;
        animation: fadeIn 0.5s ease;
    }
    
    .card-details p {
        margin-bottom: 25px; /* 增加段落之间的间距 */
        line-height: 1.8;
    }

    .research-card:target .card-details { display: block; }
    .research-card:target .read-more { display: none; }
    .research-card:target { border-color: var(--primary-blue); box-shadow: 0 12px 40px rgba(0,0,0,0.12); }

    .content-subtitle { font-size: 1.4rem; color: var(--primary-blue); margin: 40px 0 20px; font-weight: 600; } /* 增加子标题上下间隙 */
    .media-grid { display: flex; gap: 20px; margin: 30px 0; } /* 增加视频之间的间距 */
    .media-grid video { width: 32%; border-radius: 8px; }

    .research-images { display: flex; gap: 25px; margin: 30px 0; flex-wrap: wrap; }
    .research-images img { flex: 1; min-width: 150px; max-width: calc(33% - 10px); border-radius: 8px; }

    @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }

    @media (max-width: 768px) {
        .card-main { flex-direction: column; }
        .card-image { width: 100%; height: 180px; }
        .media-grid video { width: 100%; }
    }
</style>
